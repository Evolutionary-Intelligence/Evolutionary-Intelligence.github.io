---
layout: post
title: Large Language Models
subtitle: "LLMs"
cover-img: /assets/img/nationalgeographic_MelanieHaiken_NickDunlop.png
thumbnail-img: /assets/img/nationalgeographic_MelanieHaiken_NickDunlop.png
share-img: /assets/img/nationalgeographic_MelanieHaiken_NickDunlop.png
tags: [LLMs]
author: Qiqi Duan
---

- Binz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C.T., Allen, C., Schad, D., Wulff, D., West, J.D., Zhang, Q. and Shiffrin, R.M., 2025.
  How should the advancement of large language models affect the practice of science?.
  Proceedings of the National Academy of Sciences, 122(5), p.e2401227121.
- Cao, B., Cai, D., Zhang, Z., Zou, Y. and Lam, W., 2025.
  On the worst prompt performance of large language models.
  Advances in Neural Information Processing Systems, 37, pp.69022-69042.
- Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R. and Gal, Y., 2024.
  AI models collapse when trained on recursively generated data.
  Nature, 631(8022), pp.755-759.
- Sclar, M., Choi, Y., Tsvetkov, Y. and Suhr, A., 2024.
  Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting.
  In International Conference on Learning Representations.
- White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J. and Schmidt, D.C., 2023.
  A prompt pattern catalog to enhance prompt engineering with chatgpt.
  arXiv preprint arXiv:2302.11382.
- https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
- Zhou, Y., Muresanu, A.I., Han, Z., Paster, K., Pitis, S., Chan, H. and Ba, J., 2023, November.
  Large language models are human-level prompt engineers.
  In International Conference on Learning Representations.
- Prasad, A., Hase, P., Zhou, X. and Bansal, M., 2023, May.
  GrIPS: Gradient-free, edit-based instruction search for prompting large language models.
  In Proceedings of Conference of European Chapter of the Association for Computational Linguistics (pp. 3845-3864).
- Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A. and Hubert, T., 2022.
  Competition-level code generation with alphacode.
  Science, 378(6624), pp.1092-1097.
- Sun, T., Shao, Y., Qian, H., Huang, X. and Qiu, X., 2022, June.
  Black-box tuning for language-model-as-a-service.
  In International Conference on Machine Learning (pp. 20841-20855). PMLR.
- Li, X.L. and Liang, P., 2021.
  Prefix-tuning: Optimizing continuous prompts for generation.
  In Proceedings of Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (Volume 1: Long Papers). ACL.
- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever, I., 2019.
  Language models are unsupervised multitask learners.
  OpenAI Blog.
- Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C., 2003.
  A neural probabilistic language model.
  Journal of Machine Learning Research, 3(Feb), pp.1137-1155.

Copyright of the Background Figure:
[https://www.nationalgeographic.com/animals/article/these-birds-flock-in-mesmerizing-swarms-why-is-still-a-mystery](https://www.nationalgeographic.com/animals/article/these-birds-flock-in-mesmerizing-swarms-why-is-still-a-mystery).

![visitors](https://visitor-badge.laobi.icu/badge?page_id=Evolutionary-Intelligence.DistributedEvolutionaryComputation):
A summary of total number of visitors for
[Evolutionary-Intelligence.github.io](https://evolutionary-intelligence.github.io/)
(News-Focused) and
[DistributedEvolutionaryComputation](https://github.com/Evolutionary-Intelligence/DistributedEvolutionaryComputation)
(Paper-Focused).
